services:
  # from Flink docs
  # https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/#flink-with-docker-compose
  jobmanager:
    build:
      context: .
      dockerfile: config/Dockerfile.flink
    hostname: jobmanager
    container_name: jobmanager
    ports:
      - 8081:8081
      - 9249:9249
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8081"]
      interval: 3s
      timeout: 3s
      retries: 5
    depends_on:
      producer:
        condition: service_healthy
    profiles: ["flink"]
  taskmanager:
    build:
      context: .
      dockerfile: config/Dockerfile.flink
    hostname: taskmanager
    container_name: taskmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 1
    volumes:
      - ./Results:/opt/flink/output
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://jobmanager:8081"]
      interval: 3s
      timeout: 3s
      retries: 5
    depends_on:
      jobmanager: # which depends on NiFi and Kafka broker
        condition: service_healthy
    profiles: ["flink"]
  # from Kafka single-node example https://github.com/apache/kafka/blob/trunk/docker/examples/README.md
  broker:
    image: apache/kafka:3.7.0
    hostname: broker
    container_name: broker
    ports:
      - 9092:9092
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://localhost:9092,PLAINTEXT://broker:19092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_LISTENERS: 'CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: '4L6g3nShT-eMCtK--X86sw'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 3s
      timeout: 3s
      retries: 5
  nifi:
    image: apache/nifi
    container_name: nifi
    hostname: nifi
    environment:
      SINGLE_USER_CREDENTIALS_USERNAME: ${NIFI_USERNAME}
      SINGLE_USER_CREDENTIALS_PASSWORD: ${NIFI_PASSWORD}
      NIFI_WEB_HTTPS_HOST: 0.0.0.0
    ports:
      - 8443:8443
    healthcheck:
      test: ["CMD", "curl", "-k", "https://localhost:8443/nifi-api/system-diagnostics"]
      interval: 10s
      timeout: 3s
      retries: 5
    depends_on:
      broker:
        condition: service_healthy
      redis:
        condition: service_healthy
  redis:
    image: redis
    hostname: redis
    container_name: redis
    command: redis-server
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 3s
      timeout: 3s
      retries: 5
  producer:
    build:
      context: .
      dockerfile: config/Dockerfile.producer
    container_name: producer
    hostname: producer
    environment:
      NIFI_USERNAME: ${NIFI_USERNAME}
      NIFI_PASSWORD: ${NIFI_PASSWORD}
    volumes:
      - ./config/nifi_template.xml:/app/nifi_template.xml
      - ./data/dataset.csv:/app/dataset.csv
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8888/health"]
      interval: 5s
      timeout: 3s
      retries: 5
    depends_on:
      nifi: # which depends on Kafka broker
        condition: service_healthy
  # Spark Master node
  spark-master:
    build:
      context: .
      dockerfile: config/Dockerfile.spark
    container_name: spark-master
    hostname: spark-master
    command: /bin/bash
    tty: true
    stdin_open: true
    ports:
      - 8080:8080 # web-ui
      - 4040:4040 # web-ui
    environment:
      SPARK_LOCAL_IP: spark-master
    depends_on:
      producer:
        condition: service_healthy
    profiles: ["spark"]
  # Spark Worker nodes
  spark-worker-1:
    image: spark
    container_name: spark-worker-1
    hostname: spark-worker-1
    command: /bin/bash
    tty: true
    stdin_open: true
    depends_on:
      - spark-master
    profiles: ["spark"]
  spark-worker-2:
    image: spark
    container_name: spark-worker-2
    hostname: spark-worker-2
    command: /bin/bash
    tty: true
    stdin_open: true
    depends_on:
      - spark-master
    profiles: ["spark"]
  spark-worker-3:
    image: spark
    container_name: spark-worker-3
    hostname: spark-worker-3
    command: /bin/bash
    tty: true
    stdin_open: true
    depends_on:
      - spark-master
    profiles: ["spark"]
  prometheus:
    image: prom/prometheus
    hostname: prometheus
    container_name: prometheus
    ports:
      - 9090:9090
    volumes:
      - ./config/metrics/prometheus.yml:/etc/prometheus/prometheus.yml
  grafana:
    image: grafana/grafana
    container_name: grafana
    hostname: grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USERNAME}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    ports:
      - 3000:3000
    volumes:
      - ./config/metrics/datasource.yml:/etc/grafana/provisioning/datasources/prometheus.yaml
      - ./config/metrics/dashboard.yml:/etc/grafana/provisioning/dashboards/boards.yaml
      - ./config/metrics/dashboards:/etc/grafana/dashboards
    depends_on:
      - prometheus
